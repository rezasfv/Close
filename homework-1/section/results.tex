\pagebreak
\section{Results and Discussion}

\label{sec:results}

\input{section/tableResults.tex}
In this section we provide some of the most relevant results we got during the development phase.
We are considering five principal milestones that, within many different trials, led us to improve significantly our \ac{MAP} score and the overall number of relevant documents actually retrieved.

First of all, given that we were provided with two different version of the same document's \textit{corpora}, our first idea was to try with the English version.\\ 
We noticed that the best combination of basic \ac{IR} tools were to use the \textit{Porter} stemmer, a length filter from 1 to 10, and a list of stop-word composed by some standard terms and more from the top 600 extracted from the index.
The very first big milestone, that helped us to increment the \ac{MAP} of around +3.5\%, was the \textit{JavaScript} code cleaner, since we noticed by inspection that many documents were having these type of scripts inside.\\
Always by inspecting some documents and queries, and also considering that the original collection was the French version (translated then in English), we observed that the translation was very poor: by switching to French
with just cleaning the \textit{JS} code and some other minor cleaning tools, without even using an adequate stop-list and a correct stemmer for the French language, the \ac{MAP} was increasing by +5\%. \\
2 more points were achieved with a stop list built for French in the same way we did previously for English, the \textit{FrenchLightStemFilter}~\cite{solrfrenchlightstemfilter} as stemmer, and moving the length filter from 2 to 15 (as we noticed French tends to have longer words). \\
We tried some \ac{NLP} techniques for English, in particular using \ac{PoS} techniques, to see if there were improvements, and in case apply it to our main implementation for French with an appropriate model, but results
were not interesting, and also the computing time was definitely too costly.
Another approach we tried and that carried an improvement was to use \textit{Query expansion}: first we used some generative text models to expand our queries, then we decided to weight different query scores by boosting the original one linearly with respect to the number
%of expansion used, and without boosting the expansion: this carried to us an extra MAP point.
of expansion used. This made us gain an extra \ac{MAP} point.
\newline
We try to generate the embeddings for each document based on word2vector, we use pre-trained word2vec model \textit{frWac\_no\_postag\_no\_phrase\_500\_cbow\_cut100}~\cite{fauconnier_2015} for French. Then we calculate the embedding for each document and index them as KnnFloatVectorField in lucene and use KnnFloatVectorQuery for searching the query to find the k nearest documents to the target vector according to the vectors in the given field, but the results (overall map ~0.08) are not satisfying us and everything goes worser than the case of indexing and searching without anything.
\newline
We tried to combine different similarities rather than using the classic \textit{BM25Similarity}: we tried to use the Lucene \textit{MultiSimilarity} \cite{lucenemultisimilarity}, that allows to combine equally the score of two or more similarity scores, but it does not allow to
tune the weights. Then, we tried to reimplement the \textit{MultiSimilarity} class with tuning options, but results were always lower than the standard \textit{BM25Similarity}. Some minor improvements came up by fine tuning the document-length
normalization \textit{b} parameter and the term frequency component \textit{k1} parameter of the \textit{BM25Similarity}.
\newline
The last main implementation we did, was to use some \textit{Re-ranking} techniques to improve the results of the first retrieval phase.
We tried to use the \textit{SBERT} model~\cite{reimers-2019-sentence-bert}, which is a pre-trained model for sentence embeddings, and we used it to calculate the similarity between the query and the document.
We tried to use different distance metrics such as \textit{CosineSimilarity} and \textit{ManhattanDistance}, but at the end of the day, \textit{CosineSimilarity} is much better than others.
\newline
Lastly, some minor adding were set on the \textit{Analyzer} (\cite{analyzer_subsec}) by implementing the Lucene \textit{ElisionFilter} (for French) \cite{luceneelisionfilter}, which aims to remove apostrophes articles and prepositions from tokens (for example, \textit{m'appelle} and \textit{t'appelle} become the same token).




