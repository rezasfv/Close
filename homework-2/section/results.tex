\pagebreak
\section{Results and Discussion}

\label{sec:results}

\input{section/tableResults.tex}
In this Section, we provide some of the most relevant results we got during the development phase.
We are considering five principal milestones that, within many different trials, led us to improve significantly our \ac{MAP} score and the overall number of relevant documents actually retrieved.
\subsection{Results on Training Data}
First of all, given that we were provided with two different versions of the same document's \textit{corpora}, our first idea was to try the English version.\\
We noticed that the best combination of basic \ac{IR} tools was to use the \textit{Porter} Stemmer \cite{solrporterstemfilter}, a length filter from 1 to 10, and a list of stop-word composed by some standard terms and more from the top 600 extracted from the index.
The very first big milestone, that helped us to increment the \ac{MAP} of around 3 points, from 10.1\% to 13.1\%, was the \textit{JavaScript} code cleaner since we noticed by inspection that many documents were having these types of scripts inside.\\
Always by inspecting some documents and queries, and also considering that the original collection was the French version (translated then in English), we observed that the translation was very poor: by switching to French
by just cleaning the \textit{JS} code and some other minor cleaning tools, without even using an adequate stop-list and a correct stemmer for the French language, the \ac{MAP} was increasing by +5\%. \\
2 more \ac{MAP} points were achieved with a stop list built for French in the same way we did previously for English, the \textit{FrenchLightStemFilter}~\cite{solrfrenchlightstemfilter} as stemmer, and moving the length filter from 2 to 15 (as we noticed French tends to have longer words). \\
We tried some \ac{NLP} techniques for English to see if there were improvements, and in this case, apply them to our main implementation for French with an appropriate model. The obtained results were not interesting, and also the computing time was definitely too costly. In particular, we tried to use Solr OpenNLP Part of Speech Filter \cite{solropennlpposfilter} using the \textit{en-pos-maxent} \ac{PoS} tagger provided by \textit{OpenNLP}.
Another approach we tried and that carried an improvement was to use \textit{Query expansion}: first we used some generative text models to expand our queries, then we decided to weight different query scores by boosting the original one linearly with respect to the number
%of expansion used, and without boosting the expansion: this carried to us an extra MAP point.
of expansion used. This made us gain an extra \ac{MAP} point.

\newpage
\enlargethispage{4\baselineskip}
\begin{figure}[h!]
    \includegraphics[width=\textwidth]{figure/PRgraph.png}
    \caption{Standard Recall Levels vs Interpolated Precision}
    \label{fig:recallPrecision}
\end{figure}

We try to generate the embeddings for each document based on word2vector, we use a pre-trained word2vec model \textit{frWac\_no\_postag\_no\_phrase\_500\_cbow\_cut100}~\cite{fauconnier_2015} for French. Then we calculate the embedding for each document and index them as KnnFloatVectorField in Lucene and use KnnFloatVectorQuery \cite{lucene-knnvectorfield} for searching the query to find the \textit{k} nearest documents to the target vector according to the vectors in the given field, but the results (overall \ac{MAP} ~0.08) were not satisfying, being worse than the case of indexing and searching without embeddings.
\newline
We then tried to combine different similarities rather than using the classic \textit{BM25Similarity}: we tried to use the Lucene \textit{MultiSimilarity} \cite{lucenemultisimilarity}, that allows combining the score of two or more similarity scores, but it does not allow to
tune the weights. Then, we tried to reimplement the \textit{MultiSimilarity} class with tuning options, but the results were always lower than the standard \textit{BM25Similarity}. Some minor improvements came up by fine-tuning the document-length
normalization \textit{b} parameter and the term frequency component \textit{k1} parameter of the \textit{BM25Similarity}.
\newline
The last main implementation we did, was to use some \textit{Re-ranking} techniques to improve the results of the first retrieval phase.
We tried to use the \textit{SBERT} model~\cite{reimers-2019-sentence-bert}, which is a pre-trained model for sentence embeddings, and we used it to calculate the similarity between the query and the document.
We tried to use different distance metrics such as \textit{CosineSimilarity} \cite{pytorch-cosinesimilarity} and \textit{ManhattanDistance} \cite{dads-manhattandistance} for calculating the similarity, but at the end of the day, \textit{CosineSimilarity} is much better than others.
Finally, we sort the documents based on merging the BM25 score and similarity score into one score by multiplying them together.

Lastly, some minor adding were set on the \textit{Analyzer} (see Section \ref{analyzer_subsec}) by implementing the Lucene \textit{ElisionFilter} (for French) \cite{luceneelisionfilter}, which aims to remove apostrophes articles and prepositions from tokens (for example, \textit{m'appelle} and \textit{t'appelle} become the same token \textit{appelle}).


\subsection{Results on Test Data}
\label{subsec:results_submission}
\input{section/results_submission}

\pagebreak
